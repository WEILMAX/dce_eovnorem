{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from src.models.metrics import calculate_aic_bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'nw2'\n",
    "turbine = 'c02'\n",
    "mode = 'SS2'\n",
    "\n",
    "# GET THE DATA\n",
    "package_folder = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "data_folder = os.path.join(package_folder, 'data')\n",
    "models_folder = os.path.join(package_folder, 'models')\n",
    "ss2_selected = pd.read_csv(os.path.join(data_folder, 'processed','nw2', turbine+'_ss2_selected_data_large.csv'))\n",
    "ss2_selected['timestamp'] = pd.to_datetime(ss2_selected['timestamp'])\n",
    "ss2_selected.set_index('timestamp', inplace=True)\n",
    "\n",
    "SS1_dbscan = pd.read_parquet(os.path.join(data_folder, 'interim',loc,'tracked_modes', 'dbscan_based', loc+turbine+'_SS1_mode.parquet'))\n",
    "SS2_dbscan = pd.read_parquet(os.path.join(data_folder, 'interim',loc,'tracked_modes', 'dbscan_based', loc+turbine+'_SS2_mode.parquet'))\n",
    "FA1_dbscan = pd.read_parquet(os.path.join(data_folder, 'interim',loc,'tracked_modes', 'dbscan_based', loc+turbine+'_FA1_mode.parquet'))\n",
    "FA2_dbscan = pd.read_parquet(os.path.join(data_folder, 'interim',loc,'tracked_modes', 'dbscan_based', loc+turbine+'_FA2_mode.parquet'))\n",
    "\n",
    "rfe_selected_data = pd.read_parquet(os.path.join(data_folder, 'interim', loc, 'rfe_selected_data', loc+turbine+'_rfe_selected_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mvbc_WandelaarBuoy_10%_highest_waves        1.000000\n",
       "mvbc_WandelaarBuoy_Wave_height              1.000000\n",
       "mvbc_WandelaarBuoy_Sea_water_temperature    0.941740\n",
       "mvbc_WandelaarMeasuringpile_Tide_TAW        0.989682\n",
       "mvbc_WandelaarMeasuringpile_Air_pressure    0.999605\n",
       "mean_NW2_C02_rpm                            0.973195\n",
       "mean_NW2_C02_yaw                            0.973195\n",
       "mean_NW2_C02_pitch                          0.973195\n",
       "mean_NW2_C02_power                          0.973195\n",
       "mean_NW2_C02_winddirection                  0.973195\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data availability\n",
    "1 - rfe_selected_data.isna().sum()/len(rfe_selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepare the training and test data\n",
    "\n",
    "#choose y_ to be SS1_dbscan but uniquely indexed keeping the index with hghest value in size column when duplicated\n",
    "y_ = ss2_selected.copy()\n",
    "y_ = y_.sort_values(by=['size'], ascending=False)\n",
    "y_ = y_.loc[~y_.index.duplicated(keep='last')]\n",
    "y_ = y_.sort_index()\n",
    "\n",
    "#Synchronize data\n",
    "Xy = pd.DataFrame(y_['mean_frequency'])\n",
    "for col in rfe_selected_data.columns:\n",
    "    Xy[col] = rfe_selected_data[col]\n",
    "Xy.dropna(inplace=True)\n",
    "y = Xy.iloc[:,0]\n",
    "X_ = Xy[rfe_selected_data.columns]\n",
    "\n",
    "#preprocess the data\n",
    "from src.data.preprocessing import sin_cos_angle_inputs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = sin_cos_angle_inputs(X_)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "# MinMaxnormalization of the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 4.7186863529044845e-05 R2: 0.7647044697673796\n",
      "Linear Regression AIC: -6406.5367113448565 BIC: -6344.062318150436\n"
     ]
    }
   ],
   "source": [
    "# Standard linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_regr = LinearRegression()\n",
    "lin_regr.fit(X_train, y_train)\n",
    "lin_regr_pred = pd.DataFrame(lin_regr.predict(X_test), index=y_test.index, columns=['predictions'])\n",
    "lin_regr_aic, lin_regr_bic = calculate_aic_bic(lin_regr, X_test, y_test)\n",
    "print('Linear Regression', 'MSE: '+str(mean_squared_error(y_test, lin_regr_pred)), 'R2: '+str(r2_score(y_test, lin_regr_pred)))\n",
    "print('Linear Regression', 'AIC: '+str(lin_regr_aic), 'BIC: '+str(lin_regr_bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required by LinearRegression. must be real number, not str\n",
      "Case High wind: Turbine reducing output power at extreme wind speeds failed\n",
      "nan\n",
      "Found array with 0 sample(s) (shape=(0, 12)) while a minimum of 1 is required by LinearRegression. must be real number, not str\n",
      "Predicting Case Rated RPM: Turbine rotating at 10.4rpm or 10.445rpm failed\n",
      "'High wind: Turbine reducing output power at extreme wind speeds' must be real number, not str\n",
      "Predicting Case High wind: Turbine reducing output power at extreme wind speeds failed\n",
      "Multivariate Linear Regression MSE: 4.4374831504075696e-05 R2: 0.7787265622919008\n",
      "Multivariate Linear Regression AIC: -6313.328943995884 BIC: -6055.3617312182\n"
     ]
    }
   ],
   "source": [
    "# Multivariate Linear Regression\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mult_lin_regressions = {}\n",
    "mult_lin_regr_pred = pd.DataFrame(index=y_test.index, columns=['predictions'])\n",
    "case_lin_regr_pred = {}\n",
    "caseIDs = pd.read_csv(os.path.join(data_folder, 'interim', 'nw2', 'labeled', loc+turbine+'_case.csv'))\n",
    "caseIDs.set_index('timestamp', inplace=True)\n",
    "caseIDs.index = pd.to_datetime(caseIDs.index, utc=True)\n",
    "aic_mult_lin_regr = 0\n",
    "bic_mult_lin_regr = 0\n",
    "for case_ in caseIDs['caseID'].unique():\n",
    "    try:\n",
    "        case_lin_regr = LinearRegression()\n",
    "        case_index = caseIDs[caseIDs['caseID']==case_].index\n",
    "        case_index_train = case_index[case_index.isin(X_train.index)]\n",
    "        case_index_test = case_index[case_index.isin(X_test.index)]\n",
    "        case_lin_regr.fit(X_train.loc[case_index_train], y_train.loc[case_index_train])\n",
    "        mult_lin_regressions[case_] = case_lin_regr\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            if math.isnan(case_):\n",
    "                case_lin_regr = LinearRegression()\n",
    "                case_index = caseIDs[caseIDs['caseID'].isna()].index\n",
    "                case_index_train = case_index[case_index.isin(X_train.index)]\n",
    "                case_index_test = case_index[case_index.isin(X_test.index)]\n",
    "                case_lin_regr.fit(X_train.loc[case_index_train], y_train.loc[case_index_train])\n",
    "                mult_lin_regressions[case_] = case_lin_regr\n",
    "        except Exception as e2:\n",
    "            print(e1, e2)\n",
    "            print('Case '+str(case_)+' failed')\n",
    "\n",
    "mult_lin_regr_pred = pd.DataFrame(columns=['predictions'])\n",
    "for case_ in caseIDs['caseID'].unique():\n",
    "    try:\n",
    "        case_index = caseIDs[caseIDs['caseID']==case_].index\n",
    "        case_index_test = case_index[case_index.isin(X_test.index)]\n",
    "        mult_lin_regr_pred = \\\n",
    "            pd.concat(\n",
    "                [\n",
    "                mult_lin_regr_pred,\n",
    "                pd.DataFrame(mult_lin_regressions[case_].predict(X_test.loc[case_index_test]),\n",
    "                             index=X_test.loc[case_index_test].index,\n",
    "                             columns=['predictions'])\n",
    "                ],\n",
    "                axis=0)\n",
    "        aic_mult_lin_regr_case, bic_mult_lin_regr_case = \\\n",
    "            calculate_aic_bic(mult_lin_regressions[case_], X_test.loc[case_index_test], y_test.loc[case_index_test])\n",
    "        aic_mult_lin_regr += aic_mult_lin_regr_case\n",
    "        bic_mult_lin_regr += bic_mult_lin_regr_case\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            if math.isnan(case_):\n",
    "                print(case_)\n",
    "                case_index = caseIDs[caseIDs['caseID'].isna()].index\n",
    "                case_index_test = case_index[case_index.isin(X_test.index)]\n",
    "                mult_lin_regr_pred = \\\n",
    "                    pd.concat(\n",
    "                        [\n",
    "                        mult_lin_regr_pred,\n",
    "                        pd.DataFrame(mult_lin_regressions[case_].predict(X_test.loc[case_index_test]), index=X_test.loc[case_index_test].index, columns=['predictions'])\n",
    "                        ],\n",
    "                        axis=0)\n",
    "                aic_mult_lin_regr_case, bic_mult_lin_regr_case = \\\n",
    "                    calculate_aic_bic(mult_lin_regressions[case_], X_test.loc[case_index_test], y_test.loc[case_index_test])\n",
    "                aic_mult_lin_regr += aic_mult_lin_regr_case\n",
    "                bic_mult_lin_regr += bic_mult_lin_regr_case\n",
    "        except Exception as e2:\n",
    "            print(e1, e2)\n",
    "            print('Predicting Case '+str(case_)+' failed')\n",
    "mult_lin_regr_pred.sort_index(inplace=True)\n",
    "print('Multivariate Linear Regression',\n",
    "      'MSE: '+str(mean_squared_error(y_test.loc[mult_lin_regr_pred.index], mult_lin_regr_pred)),\n",
    "      'R2: '+str(r2_score(y_test.loc[mult_lin_regr_pred.index], mult_lin_regr_pred)))\n",
    "print('Multivariate Linear Regression', 'AIC: '+str(aic_mult_lin_regr), 'BIC: '+str(bic_mult_lin_regr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-optimized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressor' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m rf_regr\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      5\u001b[0m rf_regr_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rf_regr\u001b[38;5;241m.\u001b[39mpredict(X_test), index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m aic_rf_regr, bic_rf_regr \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_aic_bic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf_regr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mean_squared_error(y_test, rf_regr_pred)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(r2_score(y_test, rf_regr_pred)))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(aic_rf_regr), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(bic_rf_regr))\n",
      "File \u001b[1;32mc:\\users\\max\\documents\\owi_lab\\code\\dce_eovnorm\\dce_eov_normalisation\\src\\models\\metrics.py:8\u001b[0m, in \u001b[0;36mcalculate_aic_bic\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m rss \u001b[38;5;241m=\u001b[39m mean_squared_error(y, predictions) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# Residual Sum of Squares\u001b[39;00m\n\u001b[0;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# number of observations\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# number of parameters (coefficients + intercept)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate Log-Likelihood\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(rss\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestRegressor' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_regr = RandomForestRegressor()\n",
    "rf_regr.fit(X_train, y_train)\n",
    "rf_regr_pred = pd.DataFrame(rf_regr.predict(X_test), index=y_test.index, columns=['predictions'])\n",
    "print('Random Forest Regression', 'MSE: '+str(mean_squared_error(y_test, rf_regr_pred)), 'R2: '+str(r2_score(y_test, rf_regr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regression MSE: 4.3271484614673016e-05 R2: 0.7842283602915224\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Coefficients are not defined for Booster type None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_regr_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(xgb_regr\u001b[38;5;241m.\u001b[39mpredict(X_test), index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mean_squared_error(y_test, xgb_regr_pred)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(r2_score(y_test, xgb_regr_pred)))\n\u001b[1;32m----> 7\u001b[0m aic_xgb_regr, bic_xgb_regr \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_aic_bic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_regr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(aic_xgb_regr), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(bic_xgb_regr))\n",
      "File \u001b[1;32mc:\\users\\max\\documents\\owi_lab\\code\\dce_eovnorm\\dce_eov_normalisation\\src\\models\\metrics.py:8\u001b[0m, in \u001b[0;36mcalculate_aic_bic\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m rss \u001b[38;5;241m=\u001b[39m mean_squared_error(y, predictions) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# Residual Sum of Squares\u001b[39;00m\n\u001b[0;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# number of observations\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# number of parameters (coefficients + intercept)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate Log-Likelihood\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(rss\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\xgboost\\sklearn.py:1350\u001b[0m, in \u001b[0;36mXGBModel.coef_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03mCoefficients property\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;124;03mcoef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbooster\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients are not defined for Booster type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1352\u001b[0m     )\n\u001b[0;32m   1353\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_booster()\n\u001b[0;32m   1354\u001b[0m coef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(json\u001b[38;5;241m.\u001b[39mloads(b\u001b[38;5;241m.\u001b[39mget_dump(dump_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: Coefficients are not defined for Booster type None"
     ]
    }
   ],
   "source": [
    "# XGBoost regression\n",
    "from xgboost import XGBRegressor\n",
    "xgb_regr = XGBRegressor()\n",
    "xgb_regr.fit(X_train, y_train)\n",
    "xgb_regr_pred = pd.DataFrame(xgb_regr.predict(X_test), index=y_test.index, columns=['predictions'])\n",
    "print('XGBoost Regression', 'MSE: '+str(mean_squared_error(y_test, xgb_regr_pred)), 'R2: '+str(r2_score(y_test, xgb_regr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Regression MSE: 3.810013697163858e-05 R2: 0.8100150919088088\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CatBoostRegressor' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m cat_regr_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(cat_regr\u001b[38;5;241m.\u001b[39mpredict(X_test), index\u001b[38;5;241m=\u001b[39my_test\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mean_squared_error(y_test, cat_regr_pred)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(r2_score(y_test, cat_regr_pred)))\n\u001b[1;32m----> 7\u001b[0m aic_cb_regr, bic_cb_regr \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_aic_bic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_regr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost Regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(aic_cb_regr), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBIC: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(bic_cb_regr))\n",
      "File \u001b[1;32mc:\\users\\max\\documents\\owi_lab\\code\\dce_eovnorm\\dce_eov_normalisation\\src\\models\\metrics.py:8\u001b[0m, in \u001b[0;36mcalculate_aic_bic\u001b[1;34m(model, X, y)\u001b[0m\n\u001b[0;32m      6\u001b[0m rss \u001b[38;5;241m=\u001b[39m mean_squared_error(y, predictions) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# Residual Sum of Squares\u001b[39;00m\n\u001b[0;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)  \u001b[38;5;66;03m# number of observations\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# number of parameters (coefficients + intercept)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate Log-Likelihood\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mn\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(rss\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CatBoostRegressor' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "# CatBoost regression\n",
    "from catboost import CatBoostRegressor\n",
    "cat_regr = CatBoostRegressor()\n",
    "cat_regr.fit(X_train, y_train, verbose=False)\n",
    "cat_regr_pred = pd.DataFrame(cat_regr.predict(X_test), index=y_test.index, columns=['predictions'])\n",
    "print('CatBoost Regression', 'MSE: '+str(mean_squared_error(y_test, cat_regr_pred)), 'R2: '+str(r2_score(y_test, cat_regr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 45.\n",
      "Epoch 55: early stopping\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Neural Network 1 hidden layer MSE: 6.594846034725816e-05 R2: 0.6711504689036671\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(y_train.values.reshape(-1,1))\n",
    "y_train_scaled = target_scaler.transform(y_train.values.reshape(-1,1))\n",
    "y_test_scaled = target_scaler.transform(y_test.values.reshape(-1,1))\n",
    "\n",
    "# Define the early stopping criterion\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_squared_error',  # Monitor the validation mean squared error\n",
    "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=1e-07,  # Minimum change to qualify as an improvement\n",
    "    mode='min',  # Stop training when the quantity monitored has stopped decreasing\n",
    "    verbose=1,\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    ")\n",
    "\n",
    "def build_model(layers_=[100]):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=layers_[0], activation='relu', input_shape=(len(X.columns),)))\n",
    "    if len(layers_)>1:\n",
    "        for layer in layers_[1:]:\n",
    "            model.add(Dense(units=layer, activation='relu'))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "nn_regr = build_model()\n",
    "nn_regr.fit(X_train_scaled, y_train, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_pred = pd.DataFrame(nn_regr.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "print('Neural Network 1 hidden layer', 'MSE: '+str(mean_squared_error(y_test, nn_regr_pred)), 'R2: '+str(r2_score(y_test, nn_regr_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 32.\n",
      "Epoch 42: early stopping\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Neural Network 1 hidden layer MSE: 3.880507107857629e-05 R2: 0.8064999643485955\n"
     ]
    }
   ],
   "source": [
    "nn_regr = build_model()\n",
    "nn_regr.fit(X_train_scaled, y_train_scaled, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_pred = pd.DataFrame(nn_regr.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "nn_regr_pred_unscaled = target_scaler.inverse_transform(nn_regr_pred)\n",
    "print('Neural Network 1 hidden layer', 'MSE: '+str(mean_squared_error(y_test, nn_regr_pred_unscaled)), 'R2: '+str(r2_score(y_test, nn_regr_pred_unscaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 48.\n",
      "Epoch 58: early stopping\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "Neural Network 2 hidden layers MSE: 6.111154754731323e-05 R2: 0.6952695536835116\n"
     ]
    }
   ],
   "source": [
    "nn_regr_2 = build_model([100, 100])\n",
    "nn_regr_2.fit(X_train_scaled, y_train, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_2_pred = pd.DataFrame(nn_regr_2.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "print('Neural Network 2 hidden layers', 'MSE: '+str(mean_squared_error(y_test, nn_regr_2_pred)), 'R2: '+str(r2_score(y_test, nn_regr_2_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 21.\n",
      "Epoch 31: early stopping\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Neural Network 2 hidden layers MSE: 4.0696536142727854e-05 R2: 0.797068244545645\n"
     ]
    }
   ],
   "source": [
    "nn_regr_2 = build_model([100, 100])\n",
    "nn_regr_2.fit(X_train_scaled, y_train_scaled, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_2_pred = pd.DataFrame(nn_regr_2.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "nn_regr_2_pred_unscaled = target_scaler.inverse_transform(nn_regr_2_pred)\n",
    "print('Neural Network 2 hidden layers', 'MSE: '+str(mean_squared_error(y_test, nn_regr_2_pred_unscaled)), 'R2: '+str(r2_score(y_test, nn_regr_2_pred_unscaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 36.\n",
      "Epoch 46: early stopping\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "Neural Network 3 hidden layers MSE: 6.667698598409383e-05 R2: 0.6675177030619239\n"
     ]
    }
   ],
   "source": [
    "nn_regr_3 = build_model([100, 100, 100])\n",
    "nn_regr_3.fit(X_train_scaled, y_train, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_3_pred = pd.DataFrame(nn_regr_3.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "print('Neural Network 3 hidden layers', 'MSE: '+str(mean_squared_error(y_test, nn_regr_3_pred)), 'R2: '+str(r2_score(y_test, nn_regr_3_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Epoch 19: early stopping\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "Neural Network 3 hidden layers MSE: 3.895821901032602e-05 R2: 0.8057362979145488\n"
     ]
    }
   ],
   "source": [
    "nn_regr_3 = build_model([100, 100, 100])\n",
    "nn_regr_3.fit(X_train_scaled, y_train_scaled, epochs=100, callbacks=[early_stopping], validation_split=0.1, verbose=0)\n",
    "nn_regr_3_pred = pd.DataFrame(nn_regr_3.predict(X_test_scaled), index=y_test.index, columns=['predictions'])\n",
    "nn_regr_3_pred_unscaled = target_scaler.inverse_transform(nn_regr_3_pred)\n",
    "print('Neural Network 3 hidden layers', 'MSE: '+str(mean_squared_error(y_test, nn_regr_3_pred_unscaled)), 'R2: '+str(r2_score(y_test, nn_regr_3_pred_unscaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [19:06<00:00, 22.93s/trial, best loss: 4.373629708261872e-05]   \n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter optimization: Bayesian optimization\n",
    "#Hyperopt functions for hyperparameter optimizations\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from src.models.utils import convert_dict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "XGB_optimizations = {}\n",
    "mode = 'SS2'\n",
    "\n",
    "seed = 42\n",
    "def objective_xgb(space):\n",
    "    model = XGBRegressor(\n",
    "                                 n_estimators = space['n_estimators'],\n",
    "                                 max_depth = space['max_depth'],\n",
    "                                 learning_rate = space['learning_rate'],\n",
    "                                 colsample_bytree = space['colsample_bytree'],\n",
    "                                 )\n",
    "    score = cross_val_score(model,  X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    # We aim to minimize mse \n",
    "    return {'loss': -score, 'status': STATUS_OK }\n",
    "def optimize_xgb(trial):\n",
    "    space = {\n",
    "        'n_estimators':hp.uniformint('n_estimators',10,1000),\n",
    "        'max_depth':hp.uniformint('max_depth',2,20),\n",
    "        'learning_rate':hp.uniform('learning_rate',0.001,0.5),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree',0.1, 1),\n",
    "    }\n",
    "    best = \\\n",
    "        fmin(\n",
    "            fn = objective_xgb,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            trials = trial,\n",
    "            max_evals = 50,\n",
    "            rstate = np.random.RandomState(seed)\n",
    "            )\n",
    "    return best\n",
    "trial2=Trials()\n",
    "XGB_optimizations[mode] = optimize_xgb(trial2)\n",
    "XGB_optimizations[mode] = convert_dict(XGB_optimizations)[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor Optimized MSE: 3.854262152372167e-05 XGBRegressor Optimized R2: 0.8078086592384525\n"
     ]
    }
   ],
   "source": [
    "XGB_optimize = XGB_optimizations[mode]\n",
    "regr_xgb_optimized = \\\n",
    "    XGBRegressor(\n",
    "        n_estimators = XGB_optimize['n_estimators'],\n",
    "        max_depth = XGB_optimize['max_depth'],\n",
    "        learning_rate = XGB_optimize['learning_rate'],\n",
    "        colsample_bytree = XGB_optimize['colsample_bytree'],\n",
    "        )\n",
    "regr_xgb_optimized.fit(X_train, y_train)\n",
    "\n",
    "regr_xgb_optimized_pred = regr_xgb_optimized.predict(X_test)\n",
    "regr_xgb_optimized_mse = mean_squared_error(y_test, regr_xgb_optimized_pred)\n",
    "regr_xgb_optimized_r2 = r2_score(y_test, regr_xgb_optimized_pred)\n",
    "print(\"XGBRegressor Optimized MSE:\", regr_xgb_optimized_mse, \"XGBRegressor Optimized R2:\", regr_xgb_optimized_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write optimal hyperparameters to a csv file\n",
    "xgb_optimal_params = pd.DataFrame(XGB_optimizations, index=['optimal_params']).T\n",
    "xgb_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'xgb_optimal_params.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [1:30:55<13:38:18, 1091.09s/trial, best loss: 4.3218592117254674e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     39\u001b[0m trial_catboost \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m---> 40\u001b[0m CatBoost_optimizations \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_catboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_catboost\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m CatBoost_optimizations \u001b[38;5;241m=\u001b[39m convert_dict(CatBoost_optimizations)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(CatBoost_optimizations)\n",
      "Cell \u001b[1;32mIn[64], line 28\u001b[0m, in \u001b[0;36moptimize_catboost\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_catboost\u001b[39m(trial):\n\u001b[0;32m     19\u001b[0m     space \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniformint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniformint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbagging_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m     }\n\u001b[1;32m---> 28\u001b[0m     best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective_catboost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\fmin.py:507\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    504\u001b[0m validate_loss_threshold(loss_threshold)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\base.py:682\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    550\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    289\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    172\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\hyperopt\\base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    902\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    903\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    904\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    905\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    906\u001b[0m     )\n\u001b[1;32m--> 907\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    910\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[64], line 15\u001b[0m, in \u001b[0;36mobjective_catboost\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_catboost\u001b[39m(space):\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\n\u001b[0;32m      6\u001b[0m         iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      7\u001b[0m         depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# to make CatBoost quiet\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     )\n\u001b[1;32m---> 15\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_squared_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39mscore, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\catboost\\core.py:5703\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5701\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5704\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5705\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5706\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\catboost\\core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2315\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\Max\\anaconda3\\envs\\dce_eov_norm\\Lib\\site-packages\\catboost\\core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def objective_catboost(space):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=int(space['iterations']),\n",
    "        depth=int(space['depth']),\n",
    "        learning_rate=space['learning_rate'],\n",
    "        l2_leaf_reg=space['l2_leaf_reg'],\n",
    "        border_count=int(space['border_count']),\n",
    "        random_strength=space['random_strength'],\n",
    "        bagging_temperature=space['bagging_temperature'],\n",
    "        verbose=False  # to make CatBoost quiet\n",
    "    )\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize_catboost(trial):\n",
    "    space = {\n",
    "        'iterations': hp.uniformint('iterations', 20, 1000),\n",
    "        'depth': hp.uniformint('depth', 2, 16),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.5),\n",
    "        'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n",
    "        'border_count': hp.uniformint('border_count', 32, 255),\n",
    "        'random_strength': hp.uniform('random_strength', 0, 20),\n",
    "        'bagging_temperature': hp.uniform('bagging_temperature', 0, 1)\n",
    "    }\n",
    "    best = fmin(\n",
    "        fn=objective_catboost,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trial,\n",
    "        max_evals=50,\n",
    "        rstate=np.random.RandomState(seed)\n",
    "    )\n",
    "    return best\n",
    "\n",
    "# Example usage\n",
    "trial_catboost = Trials()\n",
    "CatBoost_optimizations = optimize_catboost(trial_catboost)\n",
    "CatBoost_optimizations = convert_dict(CatBoost_optimizations)\n",
    "print(CatBoost_optimizations)\n",
    "\n",
    "# Creating and training the optimized CatBoostRegressor\n",
    "regr_catboost_optimized = CatBoostRegressor(\n",
    "    **CatBoost_optimizations,\n",
    "    verbose=False\n",
    ")\n",
    "regr_catboost_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and evaluation\n",
    "regr_catboost_optimized_pred = regr_catboost_optimized.predict(X_test)\n",
    "regr_catboost_optimized_mse = mean_squared_error(y_test, regr_catboost_optimized_pred)\n",
    "regr_catboost_optimized_r2 = r2_score(y_test, regr_catboost_optimized_pred)\n",
    "print(\"CatBoostRegressor Optimized MSE:\", regr_catboost_optimized_mse, \"CatBoostRegressor Optimized R2:\", regr_catboost_optimized_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write optimal hyperparameters to a csv file\n",
    "cb_optimal_params = pd.DataFrame(CatBoost_optimizations, index=['optimal_params']).T\n",
    "cb_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'cb_optimal_params.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.learning_rate import *\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "def create_model(hidden_layers, units, batch_normalization):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units[0], input_dim=X_train.shape[1], activation='relu'))\n",
    "    if batch_normalization:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(1, hidden_layers):\n",
    "        model.add(Dense(units[i], activation='relu'))\n",
    "        if batch_normalization:\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))  # Assuming a regression problem\n",
    "    return model\n",
    "\n",
    "def objective(params):\n",
    "    model = create_model(params['hidden_layers'], [params['units_1'], params['units_2'], params['units_3']], params['batch_normalization'])\n",
    "    # Learning rate schedulers\n",
    "    if params['lr_schedule'] == 'linear':\n",
    "        lr_scheduler = LinearLearningRateScheduler(params['start_lr'], params['end_lr'], 100)\n",
    "    elif params['lr_schedule'] == 'sinusoidal':\n",
    "        lr_scheduler = SinusoidalLearningRateScheduler(params['base_lr'], params['max_lr'], 100)\n",
    "        \n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize(hidden_layers):\n",
    "    space = {\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'units_1': hp.uniform('units_1', 32, 256),\n",
    "        'units_2': hp.uniform('units_2', 32, 256) if hidden_layers > 1 else None,\n",
    "        'units_3': hp.uniform('units_3', 32, 256) if hidden_layers > 2 else None,\n",
    "        'batch_normalization': hp.choice('batch_normalization', [False, True]),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.0001, 0.01),\n",
    "        'lr_schedule': hp.choice('lr_schedule', ['constant', 'linear', 'sinusoidal']),\n",
    "        'start_lr': hp.uniform('start_lr', 0.0001, 0.01),\n",
    "        'end_lr': hp.uniform('end_lr', 0.0001, 0.01),\n",
    "        'base_lr': hp.uniform('base_lr', 0.0001, 0.01),\n",
    "        'max_lr': hp.uniform('max_lr', 0.0001, 0.01),\n",
    "    }\n",
    "    trials = Trials()\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_1_layer = optimize(1)\n",
    "print(\"Best Hyperparameters for 1 layer:\", best_hyperparams_1_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Train nn1_optimized with optimized hyperparameters\n",
    "if best_hyperparams_1_layer['lr_schedule'] == 'constant':\n",
    "    nn1_optimized = create_model(1, [best_hyperparams_1_layer['units_1']], best_hyperparams_1_layer['batch_normalization'])\n",
    "    nn1_optimized.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=best_hyperparams_1_layer['learning_rate']))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn1_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "elif best_hyperparams_1_layer['lr_schedule'] == 'linear':\n",
    "    nn1_optimized = create_model(1, [best_hyperparams_1_layer['units_1']], best_hyperparams_1_layer['batch_normalization'])\n",
    "    lr_scheduler = LinearLearningRateScheduler(best_hyperparams_1_layer['start_lr'], best_hyperparams_1_layer['end_lr'], 100)\n",
    "    nn1_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn1_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)\n",
    "\n",
    "elif best_hyperparams_1_layer['lr_schedule'] == 'sinusoidal':\n",
    "    nn1_optimized = create_model(1, [best_hyperparams_1_layer['units_1']], best_hyperparams_1_layer['batch_normalization'])\n",
    "    lr_scheduler = SinusoidalLearningRateScheduler(best_hyperparams_1_layer['base_lr'], best_hyperparams_1_layer['max_lr'], 100)\n",
    "    nn1_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn1_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_2_layer = optimize(2)\n",
    "print(\"Best Hyperparameters for 2 hidden layers:\", best_hyperparams_2_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train nn2_optimized with optimized hyperparameters for 2 hidden layers\n",
    "if best_hyperparams_2_layer['lr_schedule'] == 'constant':\n",
    "    nn2_optimized = create_model(2, [best_hyperparams_2_layer['units_1'], best_hyperparams_2_layer['units_2']], best_hyperparams_2_layer['batch_normalization'])\n",
    "    nn2_optimized.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=best_hyperparams_2_layer['learning_rate']))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn2_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "elif best_hyperparams_2_layer['lr_schedule'] == 'linear':\n",
    "    nn2_optimized = create_model(2, [best_hyperparams_2_layer['units_1'], best_hyperparams_2_layer['units_2']], best_hyperparams_2_layer['batch_normalization'])\n",
    "    lr_scheduler = LinearLearningRateScheduler(best_hyperparams_2_layer['start_lr'], best_hyperparams_2_layer['end_lr'], 100)\n",
    "    nn2_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn2_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)\n",
    "\n",
    "elif best_hyperparams_2_layer['lr_schedule'] == 'sinusoidal':\n",
    "    nn2_optimized = create_model(2, [best_hyperparams_2_layer['units_1'], best_hyperparams_2_layer['units_2']], best_hyperparams_2_layer['batch_normalization'])\n",
    "    lr_scheduler = SinusoidalLearningRateScheduler(best_hyperparams_2_layer['base_lr'], best_hyperparams_2_layer['max_lr'], 100)\n",
    "    nn2_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn2_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_3_layer = optimize(3)\n",
    "print(\"Best Hyperparameters for 3 hidden layers:\", best_hyperparams_3_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure you have the results from the optimization for 3 hidden layers\n",
    "best_hyperparams_3_layer = optimize(3)\n",
    "print(\"Best Hyperparameters for 3 hidden layers:\", best_hyperparams_3_layer)\n",
    "\n",
    "# Train nn3_optimized with optimized hyperparameters for 3 hidden layers\n",
    "if best_hyperparams_3_layer['lr_schedule'] == 'constant':\n",
    "    nn3_optimized = create_model(3, [best_hyperparams_3_layer['units_1'], best_hyperparams_3_layer['units_2'], best_hyperparams_3_layer['units_3']], best_hyperparams_3_layer['batch_normalization'])\n",
    "    nn3_optimized.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=best_hyperparams_3_layer['learning_rate']))\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn3_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "elif best_hyperparams_3_layer['lr_schedule'] == 'linear':\n",
    "    nn3_optimized = create_model(3, [best_hyperparams_3_layer['units_1'], best_hyperparams_3_layer['units_2'], best_hyperparams_3_layer['units_3']], best_hyperparams_3_layer['batch_normalization'])\n",
    "    lr_scheduler = LinearLearningRateScheduler(best_hyperparams_3_layer['start_lr'], best_hyperparams_3_layer['end_lr'], 100)\n",
    "    nn3_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn3_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)\n",
    "\n",
    "elif best_hyperparams_3_layer['lr_schedule'] == 'sinusoidal':\n",
    "    nn3_optimized = create_model(3, [best_hyperparams_3_layer['units_1'], best_hyperparams_3_layer['units_2'], best_hyperparams_3_layer['units_3']], best_hyperparams_3_layer['batch_normalization'])\n",
    "    lr_scheduler = SinusoidalLearningRateScheduler(best_hyperparams_3_layer['base_lr'], best_hyperparams_3_layer['max_lr'], 100)\n",
    "    nn3_optimized.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    nn3_optimized.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stop, LearningRateScheduler(lr_scheduler)], verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write optimal hyperparameters to a csv file\n",
    "nn1_optimal_params = pd.DataFrame(best_hyperparams_1_layer, index=['optimal_params']).T\n",
    "nn1_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'nn1_optimal_params.csv'))\n",
    "nn2_optimal_params = pd.DataFrame(best_hyperparams_2_layer, index=['optimal_params']).T\n",
    "nn2_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'nn2_optimal_params.csv'))\n",
    "nn3_optimal_params = pd.DataFrame(best_hyperparams_3_layer, index=['optimal_params']).T\n",
    "nn3_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'nn3_optimal_params.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize global variable to track time\n",
    "start_time = time.time()\n",
    "time_limit = 2 * 60 * 60  # 2 hours in seconds\n",
    "\n",
    "def objective_rf(space):\n",
    "    global start_time\n",
    "    if (time.time() - start_time) > time_limit:\n",
    "        raise Exception(\"Time limit exceeded\")\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=int(space['n_estimators']),\n",
    "        max_depth=int(space['max_depth']),\n",
    "        min_samples_split=int(space['min_samples_split']),\n",
    "        min_samples_leaf=int(space['min_samples_leaf']),\n",
    "        max_features=space['max_features']\n",
    "    )\n",
    "\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize_rf(trial):\n",
    "    space = {\n",
    "        'n_estimators': hp.uniformint('n_estimators', 100, 1000),\n",
    "        'max_depth': hp.uniformint('max_depth', 5, 30),\n",
    "        'min_samples_split': hp.uniformint('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': hp.uniformint('min_samples_leaf', 1, 5),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']),\n",
    "    }\n",
    "\n",
    "    best = fmin(\n",
    "        fn=objective_rf,\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trial,\n",
    "        max_evals=50,  # Adjust as needed\n",
    "        rstate=np.random.RandomState(seed)\n",
    "    )\n",
    "\n",
    "    return best\n",
    "\n",
    "trial_rf = Trials()\n",
    "RF_optimizations = {}\n",
    "RF_optimizations[mode] = optimize_rf(trial_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters for RandomForestRegressor\n",
    "RF_optimize = RF_optimizations[mode]\n",
    "\n",
    "# Create and train the RandomForestRegressor with the optimized hyperparameters\n",
    "regr_rf_optimized = RandomForestRegressor(\n",
    "    n_estimators=int(RF_optimize['n_estimators']),\n",
    "    max_depth=int(RF_optimize['max_depth']),\n",
    "    min_samples_split=int(RF_optimize['min_samples_split']),\n",
    "    min_samples_leaf=int(RF_optimize['min_samples_leaf']),\n",
    "    max_features=RF_optimize['max_features'] if RF_optimize['max_features'] in ['auto', 'sqrt', 'log2'] else 'auto'\n",
    ")\n",
    "regr_rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "regr_rf_optimized_pred = regr_rf_optimized.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error and R-squared\n",
    "regr_rf_optimized_mse = mean_squared_error(y_test, regr_rf_optimized_pred)\n",
    "regr_rf_optimized_r2 = r2_score(y_test, regr_rf_optimized_pred)\n",
    "\n",
    "# Print the scores\n",
    "print(\"RandomForestRegressor Optimized MSE:\", regr_rf_optimized_mse)\n",
    "print(\"RandomForestRegressor Optimized R2:\", regr_rf_optimized_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write optimal hyperparameters to a csv file\n",
    "rf_optimal_params = pd.DataFrame(RF_optimizations, index=['optimal_params']).T\n",
    "rf_optimal_params.to_csv(os.path.join(models_folder, loc, turbine, 'hyperparameters', mode, 'rf_optimal_params.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dce_eov_norm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
